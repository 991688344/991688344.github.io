<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Attention机制_transformer | Rick</title><meta name="author" content="LYC"><meta name="copyright" content="LYC"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="self-Attention 用Q来找相关的K 计算新的具有注意力信息词汇的过程 self att的 两种不同的架构 这是计算两个word的注意力分数，WWW矩阵为一个transformer  第一个词分别和其它所有词做注意力分数计算（包括自己），最后把分数进行softmax得到最终分数  ​	WqW_qWq​相当于查询矩阵， WkW_kWk​ 相当与键值矩阵，WvW_vWv​ 相当于获取原始值矩">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention机制_transformer">
<meta property="og:url" content="http://991688344.github.io/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/index.html">
<meta property="og:site_name" content="Rick">
<meta property="og:description" content="self-Attention 用Q来找相关的K 计算新的具有注意力信息词汇的过程 self att的 两种不同的架构 这是计算两个word的注意力分数，WWW矩阵为一个transformer  第一个词分别和其它所有词做注意力分数计算（包括自己），最后把分数进行softmax得到最终分数  ​	WqW_qWq​相当于查询矩阵， WkW_kWk​ 相当与键值矩阵，WvW_vWv​ 相当于获取原始值矩">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://991688344.github.io/images/Wallpaper/eatham.gif">
<meta property="article:published_time" content="2022-11-21T11:55:15.000Z">
<meta property="article:modified_time" content="2024-03-19T01:32:36.446Z">
<meta property="article:author" content="LYC">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://991688344.github.io/images/Wallpaper/eatham.gif"><link rel="shortcut icon" href="/images/Wallpaper/favicon.ico"><link rel="canonical" href="http://991688344.github.io/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/content.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Attention机制_transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/mycss/my_background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #F7F9FE;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">251</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">49</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/Wallpaper/eatham.gif);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Rick</span></a><a class="nav-page-title" href="/"><span class="site-name">Attention机制_transformer</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Attention机制_transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-11-21T11:55:15.000Z" title="发表于 2022-11-21 19:55:15">2022-11-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-19T01:32:36.446Z" title="更新于 2024-03-19 09:32:36">2024-03-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/Attention/">Attention</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>self-Attention</h1>
<p>用Q来找相关的K</p>
<h2 id="计算新的具有注意力信息词汇的过程">计算新的具有注意力信息词汇的过程</h2>
<p>self att的 两种不同的架构</p>
<p>这是计算两个word的注意力分数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>矩阵为一个transformer</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121095355913.png" alt="image-20221121095355913"></p>
<p>第一个词分别和其它所有词做注意力分数计算（包括自己），最后把分数进行softmax得到最终分数</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121100736608.png" alt="image-20221121100736608"></p>
<p>​	<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>相当于查询矩阵， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 相当与键值矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 相当于获取原始值矩阵。<strong>这三个矩阵都是共用的，这三个矩阵来控制这几个词的互相的注意力大小</strong></p>
<p>​	每个词用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>获取自己的向量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>获取被查询词的向量，两向量进行点乘–&gt; softmax后得到相似度（注意力）分数。最后这个查询的词利用获取到的所有与其他词的注意力分数和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>获取的原始值分别相乘后相加得到一个新的向量。</p>
<p>​	这个词的新向量就包含了其它所有词的联系信息，并且联系强弱不同，即所谓的自注意力。</p>
<h3 id="计算b-1过程">计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">b_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>过程</h3>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121100926430.png" alt="image-20221121100926430"></p>
<h3 id="计算b-2过程">计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">b_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>过程</h3>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121102115542.png" alt="image-20221121102115542"></p>
<h2 id="矩阵乘法角度">矩阵乘法角度</h2>
<p>既然<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 都是共享的，那么计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">q^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0191em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>k</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">k^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">v^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span> 就可以用矩阵乘法来一次性并行计算出来</p>
<h3 id="1-计算qkv">1. 计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mi>k</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">qkv</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></h3>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121102725800.png" alt="image-20221121102725800"></p>
<h3 id="2-计算注意力分数">2. 计算注意力分数</h3>
<p>每个词汇的q，分别乘其他矩阵的k，来得到与其他词的注意力分数</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121103259054.png" alt="image-20221121103259054"></p>
<h3 id="3-计算新的包含注意力信息的词汇">3. 计算新的包含注意力信息的词汇</h3>
<p>注意力分数和原始词汇通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> transformer后相乘得到最终的新词汇</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121103549513.png" alt="image-20221121103549513"></p>
<h3 id="4-总过程">4. 总过程</h3>
<p>只有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">W_q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">W_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">W_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是需要学习的，这三个控制所有的相互之间的注意力大小</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121103802259.png" alt="image-20221121103802259"></p>
<h1>Multi-head Self-attention</h1>
<p>自注意力就是用Q来找相关的K，但是“相关”会有很多种不同的种类不同的形式，所以需要多个Q来找不同形式的相关。</p>
<h2 id="计算b1的不同形式的注意力结果">计算b1的不同形式的注意力结果</h2>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">q^{i,1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0191em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v^{i,1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>一起算，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>q</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">q^{i,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0191em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mrow><mi>i</mi><mo separator="true">,</mo><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v^{i,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>一起算</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121104818402.png" alt="image-20221121104818402"></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121105345315.png" alt="image-20221121105345315"></p>
<p>最后把这个词的不同形式注意力结果拼接起来乘上最终的一个transformer <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>o</mi></msub></mrow><annotation encoding="application/x-tex">W_o</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 得到这个词汇的最终注意力结果</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121105459645.png" alt="image-20221121105459645"></p>
<h1>Positional Encoding</h1>
<p>以上说的self attention虽然包含了其他词汇的注意力信息，但是少了个非常重要的信息–<strong>位置信息</strong>。  比如“saw a saw”这个句子中两个“saw”谁前谁后 前面提到的self attention并不知道。</p>
<p>不同的位置都有一个位置vector <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mi>i</mi></msup></mrow><annotation encoding="application/x-tex">e^i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8247em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span></span></span></span></span></span></span></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121110536613.png" alt="image-20221121110536613"></p>
<h1>self-Attention 在图像领域应用</h1>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121111656334.png" alt="image-20221121111656334"></p>
<p>f(x),g(x),h(x)就是 QKV</p>
<h2 id="1-image-20221121132625053Self-Attention-VS-CNN">1. <img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121132625053.png" alt="image-20221121132625053">Self-Attention VS CNN</h2>
<p>如果把图片做inner poduct的时候，如下图 A处的像素做Query，B处的像素做Key，每个像素考虑的就不是和CNN一样在卷积核的小范围内了，而是考虑的全局信息，所以CNN可以说是一个self-ATT的简化版本。</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121112242770.png" alt="image-20221121112242770"></p>
<p>CNN的receptive field是认为划定的，而self-ATT的receptive field是自动学习出来的。</p>
<p>下面的论文用数学的方式证明CNN就是self-attention的子集。</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121112428813.png" alt="image-20221121112428813"></p>
<p>CNN适用于数据集少的情况，不容易过拟合</p>
<h2 id="2-Self-Attention-VS-RNN">2. Self-Attention VS RNN</h2>
<p>关键的区别就是RNN的词汇要和离着非常远的词汇建立联系的话，会存在梯度消失的现象，难以训练，而Self-Attention不存在这样的问题</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121133041510.png" alt="image-20221121133041510"></p>
<p>而且RNN无法并行处理所有数据</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121133121726.png" alt="image-20221121133121726"></p>
<h2 id="3-Self-Attention-for-graph">3. Self-Attention for graph</h2>
<p>在之前的网络中，每个词汇之间有没有关联是自己学习出来的，但是在图里面，点之间的关联信息已经给出了，只需要计算有关联的点之间的注意力分数就够了。</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121133806177.png" alt="image-20221121133806177"></p>
<h1>Transformer</h1>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Xp4y1b7ih?p=3&amp;vd_source=a61337b33cfc68a14ecb4713a0602ea5">https://www.bilibili.com/video/BV1Xp4y1b7ih?p=3&amp;vd_source=a61337b33cfc68a14ecb4713a0602ea5</a></p>
<p>transformer 是个seq2seq的model</p>
<p>seq2seq有几种情况，一对一、多对一、多对多、输出长度不固定 需要网络自己学习，例如语音识别和翻译</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121134635033.png" alt="image-20221121134635033"></p>
<p>输入长度和输出长度之间的关系不是固定的</p>
<h2 id="1-transformer架构">1. transformer架构</h2>
<p>有一个编码架构，有个解码架构</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121142508466.png" alt="image-20221121142508466"></p>
<h2 id="2-Encoer部分">2. Encoer部分</h2>
<p>作用就是给一排向量 ，输出另一排向量（包含注意力信息）。可以用CNN、RNN、Self-Attention做，Transformer里面用的是Self-Attention</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121144708169.png" alt="image-20221121144708169"></p>
<p>每个Encoder里面都有很多 block（对应上图中的Nx块）</p>
<p>每个block都是先经过Self-Attention（对应<code>Multi-Head Attention</code>）然后再FC（对应 <code>Feed Forward</code>）</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121142212012.png" alt="image-20221121142212012"></p>
<p>在block里面还加入了残差的思想：经过<code>self-Att</code> 和<code>FC</code> 后先和原输入残差连接（对应<code>Add</code>），然后在进行<code>Layer Normal</code>（对应Norm）</p>
<p>注意这里的normal是layer normal 不是 batch nomral</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121142316246.png" alt="image-20221121142316246"></p>
<h3 id="To-learn-more">To learn more</h3>
<p>以上是原论文中用到的结构，也有很多改进版本：</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121144843731.png" alt="image-20221121144843731"></p>
<p>第一篇文章讨论了<code>layer Norm</code>的位置在哪更合适</p>
<p>第二篇文章讨论了什么<code>Normalization</code>更好</p>
<h2 id="3-Decoder">3. Decoder</h2>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121152638056.png" alt="image-20221121152638056"></p>
<h3 id="Autoregressive">Autoregressive</h3>
<p>依次产生每个词</p>
<h4 id="整体过程">整体过程</h4>
<p>在Decoder中，首先把Encoder的输出先读入（具体怎么读入的先不展开）</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121152847134.png" alt="image-20221121152847134"></p>
<p>首先定义两个特殊的符号<code>BOS(Begin of Sentence)</code>、<code>EOS(End of Sentence)</code></p>
<p>Decoder类似于RNN，先根据输入生成第一个词汇</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121153204040.png" alt="image-20221121153204040"></p>
<p>然后将这个词汇作为下一步的输入 … …</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121153221093.png" alt="image-20221121153221093"></p>
<p>这里存在一个问题就是会存在错误传播的问题，即一步错，步步错</p>
<h4 id="详细结构">详细结构</h4>
<p>如果把Decoder中间这部分遮住，则Encoder、Decoder两个差不多</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121153734898.png" alt="image-20221121153734898"></p>
<h4 id="Masked-Self-Attention">Masked Self-Attention</h4>
<p>区别在Decoder中有个<code>Masked</code>，那这个<code>Masked</code>是什么意思呢：</p>
<p>在原始的<code>Self-Att</code>中，每一步的输入都会考虑到后面所有步的输入</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121153859890.png" alt="image-20221121153859890"></p>
<p>在<code>Masked Self-attention</code>中，当前词汇的产生不能考虑后面的部分</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121154915460.png" alt="image-20221121154915460"></p>
<p>因为Decoder中是一个一个输出的，当前时刻并不会获得之后时刻的信息</p>
<blockquote>
<p>ps：Encoder是根据全部信息训练出来的，那么Decoder这样做会不会“不对称” ？如果实时性要求不是特别高的话，那是不是可以设置一个窗口来让当前时刻学习一下窗口长度的之后时刻的信息？</p>
</blockquote>
<p>为了让Decoder可以停下，还需要设置一个特殊符号<code>EOS</code>。</p>
<h3 id="Non-Autoregressive-NAT">Non-Autoregressive (NAT)</h3>
<p>一次产生所有词</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121160622669.png" alt="image-20221121160622669"></p>
<h2 id="4-Encoder-Decoder之间的传递">4. Encoder-Decoder之间的传递</h2>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121160814523.png" alt="image-20221121160814523"></p>
<p>Decoder将输入进行Att后得到的结果当作<code>Query</code>，Encoder中的输出作为<code>Key</code>、<code>Value</code>进行计算</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121161030680.png" alt="image-20221121161030680"></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121161255011.png" alt="image-20221121161255011"></p>
<p>下面是使用seq2seq做语音识别的文章，使用到了Cross Att思想</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121161805694.png" alt="image-20221121161805694"></p>
<p>首先上面的声音列向量输入到Encoder中，然后decoder输出与Encoder输出做cross Att</p>
<h2 id="5-Learn-To-more">5. Learn To more</h2>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121164644164.png" alt="image-20221121164644164"></p>
<h2 id="6-Training">6. Training</h2>
<p>在训练的时候会给Decoder看正确答案，也就是告诉Decoder 输入BEGIN时候要输出<code>机</code>，输入<code>机</code>的时候要输出<code>器</code> … …。 这种技术为<code>Teacher Forcing</code></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121165303236.png" alt="image-20221121165303236"></p>
<p>但是现在有个问题就是Decoder在训练过程中看到的是正确答案。真正在使用的过程中没有正确答案可以看，这其中存在一个Mismatch</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121171919670.png" alt="image-20221121171919670"></p>
<p>这就可能导致一步错 步步错的问题</p>
<p>解决方法之一就是在训练过程中给Decoder加一些错误的标签，类似于加噪声，正则化</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121172059757.png" alt="image-20221121172059757"></p>
<p>这种技术叫做 <code>Scheduled Sampling</code></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121172126260.png" alt="image-20221121172126260"></p>
<h3 id="评估">评估</h3>
<p>在训练时后的损失是单个词汇的，而评估的时候是评估整个句子</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121171019857.png" alt="image-20221121171019857"></p>
<p>那能不能training的时候就考虑整个句子呢，即training的时候使用的就是BLEU loss呢？ 可以，但是BLEU本身是很复杂，是不能微分的。</p>
<h3 id="Trick">Trick</h3>
<p><strong>遇到无法 optimization的LOSS，用RL硬Train一发就对了，把它当作是RL的</strong></p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121171755530.png" alt="image-20221121171755530"></p>
<h1>Guided Attention</h1>
<p>强迫机器把输入的每个东西都“看到”</p>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121170621544.png" alt="image-20221121170621544"></p>
<h1>To Learn more</h1>
<p><img src="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/image-20221121134304407.png" alt="image-20221121134304407"></p>
<h2 id="自注意力与交叉注意力">自注意力与交叉注意力</h2>
<h3 id="交叉注意力">交叉注意力</h3>
<p>在交叉注意力中，每个输入序列都有自己的查询向量，而输出序列的每个位置都有一个键向量。当进行交叉注意力计算时，模型会根据查询向量和键向量之间的相似度来分配权重，从而决定要将哪些信息纳入到当前位置的计算中。</p>
<p>这种权重的分配过程可以被看做是对每个输入序列中的每个位置进行的关注度量，因此我们可以通过观察这些关注度量来理解模型在进行推断时关注了哪些输入位置，并进一步解释模型的决策原因。</p>
<p>此外，在交叉注意力中，我们还可以通过可视化注意力权重矩阵来进一步解释模型的决策原因，并理解哪些输入特征对于推断结果的产生起到了决定性的作用。</p>
<p>因此，交叉注意力在深度学习模型中具有较好的可解释性，可以帮助我们更好地理解模型的决策过程和推断结果。</p>
<h3 id="交叉注意力为什么可以同时关注多个输入序列之间的依赖关系">交叉注意力为什么可以同时关注多个输入序列之间的依赖关系</h3>
<p>交叉注意力可以同时关注多个输入序列之间的依赖关系的原因在于：</p>
<ol>
<li>分别计算多个输入序列的查询向量，以及一个输出序列（假定要根据这些输入序列得到该输出序列），然后将多个查询向量联系在一起，以便同时考虑到所有输入序列与输出序列之间的关系。</li>
<li>在计算注意力权重时，会同时计算多个输入序列和输出序列之间的相似度，以便于获取每个输入序列的特征，然后将它们组合起来。</li>
<li>在每个时刻，交叉注意力都可以计算多个输入序列当前位置的特征向量，然后加权求和得到一个位置无关的向量表示。这个向量用于计算输出序列的下一个位置。</li>
</ol>
<p>因此，交叉注意力通过对多个输入序列的特征进行编码以及计算它们与输出序列之间的相似度，可以同时关注多个输入序列之间的依赖关系，并且根据它们进行更好的推理和预测。它可以应用于一些需要对多个输入序列进行建模的任务，如机器翻译、图像描述等。</p>
<h3 id="区别">区别</h3>
<p><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/49468/whats-the-difference-between-attention-vs-self-attention-what-problems-does-ea">https://datascience.stackexchange.com/questions/49468/whats-the-difference-between-attention-vs-self-attention-what-problems-does-ea</a></p>
<p><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">https://machinelearningmastery.com/the-transformer-attention-mechanism/</a></p>
<p>自注意力机制和交叉注意力机制都是注意力机制，在深度学习中都被广泛应用于序列建模任务中，但它们之间也存在一些明显的区别。</p>
<ol>
<li>输入的不同：自注意力机制仅关注单个输入序列的不同位置之间的依赖关系，而交叉注意力机制可以同时关注多个输入序列之间的依赖关系。</li>
<li>查询、键和值的不同：在自注意力机制中，查询、键和值都是从同一个输入序列中提取得到的，而在交叉注意力机制中，<strong>查询和值是从输出序列中提</strong>取得到的，键则是从另一个输入序列中提取得到的。</li>
<li>实现的不同：自注意力机制和交叉注意力机制在实现上也略有不同。在自注意力机制中，我们可以通过一个线性变换对输入序列进行映射，然后基于映射后得到的特征向量进行查询、键和值的计算。而在交叉注意力机制中，我们需要将不同的输入序列映射到同一维度上，然后进行类似的计算。</li>
<li>应用的不同：自注意力机制主要应用于单个输入序列的处理，可以用于语言建模、情感分析等任务。而交叉注意力机制则主要应用于关联不同的输入序列，可以用于机器翻译、图像描述等任务。</li>
</ol>
<p>综上所述，自注意力和交叉注意力在输入、查询、键和值的不同、实现方式和应用场景的不同，因此在具体应用时需要酌情选择。</p>
<h3 id="例子">例子</h3>
<h4 id="1-自注意力和交叉注意力-例子">1. 自注意力和交叉注意力 例子</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">自注意力机制的一个例子是Transformer架构中的编码器和解码器的每一层都有一个自注意力子层，它可以让模型学习到序列中每个位置与其他位置的关系。</a><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">1</a></li>
<li><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">交叉注意力机制的一个例子是Transformer架构中的解码器的第二个子层，它可以让模型学习到目标序列与源序列之间的对齐关系。</a><a target="_blank" rel="noopener" href="https://machinelearningmastery.com/the-transformer-attention-mechanism/">1</a></li>
</ul>
<p>下面是一个自注意力机制和交叉注意力机制的图示：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Self-attention:</span><br><span class="line"></span><br><span class="line">Q = K = V = [x1, x2, ..., xn] (a single sequence)</span><br><span class="line"></span><br><span class="line">Attention(Q, K, V) = softmax(QK^T / sqrt(d))V</span><br><span class="line"></span><br><span class="line">Cross-attention:</span><br><span class="line"></span><br><span class="line">Q = [y1, y2, ..., ym] (target sequence)</span><br><span class="line">K = V = [x1, x2, ..., xn] (source sequence)</span><br><span class="line"></span><br><span class="line">Attention(Q, K, V) = softmax(QK^T / sqrt(d))V</span><br></pre></td></tr></table></figure>
<h4 id="2-交叉注意力多模态翻译">2. 交叉注意力多模态翻译</h4>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1566253522001877">一个交叉注意力应用于多源机器翻译的例子是多模态机器翻译，它可以让模型同时使用图像和文本作为输入，生成目标语言的文本输出。</a><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S1566253522001877">1</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.00135.pdf">一个交叉注意力应用于多模态融合的例子是视频分类，它可以让模型同时使用视觉和听觉信息作为输入，生成视频类别的输出。</a><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.00135.pdf">2</a></li>
</ul>
<p>下面是一个多模态机器翻译的图示：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Image: [x1, x2, ..., xn] (image features)</span><br><span class="line">Text: [y1, y2, ..., ym] (source text tokens)</span><br><span class="line"></span><br><span class="line">Encoder: Image + Text -&gt; [z1, z2, ..., zn+m] (concatenated features)</span><br><span class="line"></span><br><span class="line">Decoder: [z1, z2, ..., zn+m] -&gt; [w1, w2, ..., wk] (target text tokens)</span><br><span class="line"></span><br><span class="line">Cross-attention: Attention([w1], [z1,...zn+m]) -&gt; a11</span><br><span class="line">               Attention([w2], [z1,...zn+m]) -&gt; a21</span><br><span class="line">               ...</span><br><span class="line">               Attention([wk], [z1,...zn+m]) -&gt; ak(n+m)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.03976">其中，Image和Text是两个不同类型的输入序列，Encoder是一个编码器网络，Decoder是一个解码器网络，Cross-attention是一个交叉注意力机制，Attention是一个注意力函数，a11,a21,…ak(n+m)是注意力权重。</a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.03976">3</a></p>
<hr>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/585255118">注意力机制作用被高估了？苹果等机构新研究：把注意力矩阵替换成常数矩阵后，性能差异不大</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104393915">【经典精读】Transformer模型和Attention机制</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_11466419/5530949">Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解</a></p>
<h1>Refer</h1>
<hr>
<p><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf">self attention PPT</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Xp4y1b7ih/?p=5&amp;spm_id_from=pageDriver&amp;vd_source=a61337b33cfc68a14ecb4713a0602ea5">台大李宏毅21年机器学习课程 self-attention和transformer</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://991688344.github.io">LYC</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://991688344.github.io/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/">http://991688344.github.io/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://991688344.github.io" target="_blank">Rick</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post-share"><div class="social-share" data-image="/images/Wallpaper/eatham.gif" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/08/27/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/" title="实验室服务器网络运维"><img class="cover" src="/images/Wallpaper/eatham.gif" onerror="onerror=null;src='/images/404.png'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">实验室服务器网络运维</div></div><div class="info-2"><div class="info-item-1">1. 网卡重命名 Linux 内核通过将固定前缀与随着内核初始化网络设备而增加的数字连接在一起，来为网络接口分配名称。例如，eth0 代表启动时被探测的第一个设备。如果您在系统中添加另一个网卡，则内核设备名称的分配将不再是固定的。因此，重启后内核可能会以不同的方式为该设备命名。 要解决这个问题，`udev` 设备管理器支持几种不同的命名方案。默认情况下，`udev` 根据固件、拓扑和位置信息分配固定名称。它有以下优点： 	   设备名称完全可预测。 即使添加或删除了硬件，设备名称也会保持不变，因为不会进行重新枚举。 因此，有问题的硬件可以被无缝地替换。  查看某个网络设备的信息 1sudo udevadm test /sys/class/net/ethlab  列出某个设备的所有属性 1udevadm info -ap...</div></div></div></a><a class="pagination-related" href="/2022/06/28/%E4%BA%8C%E7%BA%A7%E8%B7%AF%E7%94%B1ipv6%E5%9C%B0%E5%9D%80%E4%B8%8B%E5%8F%91/" title="二级路由ipv6地址下发"><img class="cover" src="/images/Wallpaper/eatham.gif" onerror="onerror=null;src='/images/404.png'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">二级路由ipv6地址下发</div></div><div class="info-2"><div class="info-item-1">首先各个 /64 的意义是不一样的，通过 SLAAC 获得的只是一个地址，里面的“前缀长度”字段（值为“/64”）只是相当于 ipv4 的子网掩码的含义，只是提示了同一网段（ WAN 侧）的地址范围，同前缀的其它地址是不属于你的。所以说当路由器仅获取 WAN 地址时（无论是通过 SLAAC 还是 DHCPv6 IA_NA，前者只支持  /64，后者支持任意的前缀长度，注意这两种方式获取到的都是“地址”而不是“前缀”），LAN 端设备是无法直接获得 IPv6 全球单播地址（就是 2 开头的公网地址）的。 然后还有另外一种操作是 DHCPv6 IA_PD，这种操作可以向上级网络请求一个 IPv6  的前缀，一般家宽运营商给出的前缀长度都是 /56 或者 /60 的，也有极少情况是 /64  的。通过这种方式获取到的是“前缀”而不是“地址”，也就是说同前缀的 2^(128-PREFIX_LEN)个地址都是属于你的。这就是 DHCPv6 IA_PD 和其它地址获取方式（ SLAAC 和 DHCPv6 IA_NA...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2019/10/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/" title="机器学习中的正则化"><img class="cover" src="/img/AI2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2019-10-13</div><div class="info-item-2">机器学习中的正则化</div></div><div class="info-2"><div class="info-item-1">关于正则化的理解 正则化是为了防止过拟合 如下图，红色这条“想象力”过于丰富上下横跳的曲线就是过拟合情形。结合上图和正则化的英文 Regularizaiton-Regular-Regularize，直译应该是：规则化（加个“化”字变动词，自豪一下中文还是强）。什么是规则？你妈喊你6点前回家吃饭，这就是规则，一个限制。同理，在这里，规则化就是说给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀。正则化，看起来，挺不好理解的，追其根源，还是“正则”这两字在中文中实在没有一个直观的对应，如果能翻译成规则化，更好理解。但我们一定要明白，搞学术，概念名词的准确是十分重要，对于一个重要唯一确定的概念，为它安上一个不会产生歧义的名词是必须的，正则化的名称没毛病，只是从如何理解的角度，要灵活和类比。  相关概念 1.强凸性 强凸性多用在优化中(Optimization)，特别是保证很多基于梯度下降方法的算法的线形收敛速率的条件之一。 一个可微函数强凸的定义是： f(y)≥f(x)+∇f(x)T(y−x)+u2∥y−x∥2f(y)≥f(x)+∇f(x)T(y−x)+u2...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC80NzA1My8yMzU1Mw=="></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LYC</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">251</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">49</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">self-Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84%E5%85%B7%E6%9C%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BF%A1%E6%81%AF%E8%AF%8D%E6%B1%87%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">1.1.</span> <span class="toc-text">计算新的具有注意力信息词汇的过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97b-1%E8%BF%87%E7%A8%8B"><span class="toc-number">1.1.1.</span> <span class="toc-text">计算b1b_1b1​过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97b-2%E8%BF%87%E7%A8%8B"><span class="toc-number">1.1.2.</span> <span class="toc-text">计算b2b_2b2​过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E8%A7%92%E5%BA%A6"><span class="toc-number">1.2.</span> <span class="toc-text">矩阵乘法角度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%A1%E7%AE%97qkv"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. 计算qkvqkvqkv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. 计算注意力分数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84%E5%8C%85%E5%90%AB%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BF%A1%E6%81%AF%E7%9A%84%E8%AF%8D%E6%B1%87"><span class="toc-number">1.2.3.</span> <span class="toc-text">3. 计算新的包含注意力信息的词汇</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%80%BB%E8%BF%87%E7%A8%8B"><span class="toc-number">1.2.4.</span> <span class="toc-text">4. 总过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Multi-head Self-attention</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97b1%E7%9A%84%E4%B8%8D%E5%90%8C%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BB%93%E6%9E%9C"><span class="toc-number">2.1.</span> <span class="toc-text">计算b1的不同形式的注意力结果</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">self-Attention 在图像领域应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-image-20221121132625053Self-Attention-VS-CNN"><span class="toc-number">4.1.</span> <span class="toc-text">1. Self-Attention VS CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Self-Attention-VS-RNN"><span class="toc-number">4.2.</span> <span class="toc-text">2. Self-Attention VS RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Self-Attention-for-graph"><span class="toc-number">4.3.</span> <span class="toc-text">3. Self-Attention for graph</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-transformer%E6%9E%B6%E6%9E%84"><span class="toc-number">5.1.</span> <span class="toc-text">1. transformer架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Encoer%E9%83%A8%E5%88%86"><span class="toc-number">5.2.</span> <span class="toc-text">2. Encoer部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#To-learn-more"><span class="toc-number">5.2.1.</span> <span class="toc-text">To learn more</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Decoder"><span class="toc-number">5.3.</span> <span class="toc-text">3. Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Autoregressive"><span class="toc-number">5.3.1.</span> <span class="toc-text">Autoregressive</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E8%BF%87%E7%A8%8B"><span class="toc-number">5.3.1.1.</span> <span class="toc-text">整体过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E7%BB%93%E6%9E%84"><span class="toc-number">5.3.1.2.</span> <span class="toc-text">详细结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Masked-Self-Attention"><span class="toc-number">5.3.1.3.</span> <span class="toc-text">Masked Self-Attention</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Non-Autoregressive-NAT"><span class="toc-number">5.3.2.</span> <span class="toc-text">Non-Autoregressive (NAT)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Encoder-Decoder%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BC%A0%E9%80%92"><span class="toc-number">5.4.</span> <span class="toc-text">4. Encoder-Decoder之间的传递</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Learn-To-more"><span class="toc-number">5.5.</span> <span class="toc-text">5. Learn To more</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Training"><span class="toc-number">5.6.</span> <span class="toc-text">6. Training</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0"><span class="toc-number">5.6.1.</span> <span class="toc-text">评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trick"><span class="toc-number">5.6.2.</span> <span class="toc-text">Trick</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">Guided Attention</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">To Learn more</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8E%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">7.1.</span> <span class="toc-text">自注意力与交叉注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">7.1.1.</span> <span class="toc-text">交叉注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AF%E4%BB%A5%E5%90%8C%E6%97%B6%E5%85%B3%E6%B3%A8%E5%A4%9A%E4%B8%AA%E8%BE%93%E5%85%A5%E5%BA%8F%E5%88%97%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="toc-number">7.1.2.</span> <span class="toc-text">交叉注意力为什么可以同时关注多个输入序列之间的依赖关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8C%BA%E5%88%AB"><span class="toc-number">7.1.3.</span> <span class="toc-text">区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">7.1.4.</span> <span class="toc-text">例子</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B-%E4%BE%8B%E5%AD%90"><span class="toc-number">7.1.4.1.</span> <span class="toc-text">1. 自注意力和交叉注意力 例子</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%A4%9A%E6%A8%A1%E6%80%81%E7%BF%BB%E8%AF%91"><span class="toc-number">7.1.4.2.</span> <span class="toc-text">2. 交叉注意力多模态翻译</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">Refer</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/08/29/4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%8F%8A%E7%BB%B4%E6%8A%A4/" title="4GPU服务器环境配置及维护"><img src="/images/Wallpaper/rainbowcat.gif" onerror="this.onerror=null;this.src='/images/404.png'" alt="4GPU服务器环境配置及维护"/></a><div class="content"><a class="title" href="/2023/08/29/4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E5%8F%8A%E7%BB%B4%E6%8A%A4/" title="4GPU服务器环境配置及维护">4GPU服务器环境配置及维护</a><time datetime="2023-08-29T12:00:26.000Z" title="发表于 2023-08-29 20:00:26">2023-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/29/UEFI-systemd%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/" title="UEFI+systemd开机启动流程"><img src="/images/Wallpaper/eatham.gif" onerror="this.onerror=null;this.src='/images/404.png'" alt="UEFI+systemd开机启动流程"/></a><div class="content"><a class="title" href="/2023/08/29/UEFI-systemd%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/" title="UEFI+systemd开机启动流程">UEFI+systemd开机启动流程</a><time datetime="2023-08-29T08:39:54.000Z" title="发表于 2023-08-29 16:39:54">2023-08-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/27/udev%E9%85%8D%E7%BD%AELinux%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3/" title="udev配置Linux网络接口"><img src="/images/Wallpaper/eatham.gif" onerror="this.onerror=null;this.src='/images/404.png'" alt="udev配置Linux网络接口"/></a><div class="content"><a class="title" href="/2023/08/27/udev%E9%85%8D%E7%BD%AELinux%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3/" title="udev配置Linux网络接口">udev配置Linux网络接口</a><time datetime="2023-08-27T12:49:29.000Z" title="发表于 2023-08-27 20:49:29">2023-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/08/27/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/" title="实验室服务器网络运维"><img src="/images/Wallpaper/eatham.gif" onerror="this.onerror=null;this.src='/images/404.png'" alt="实验室服务器网络运维"/></a><div class="content"><a class="title" href="/2023/08/27/%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%BD%91%E7%BB%9C%E8%BF%90%E7%BB%B4/" title="实验室服务器网络运维">实验室服务器网络运维</a><time datetime="2023-08-27T12:20:23.000Z" title="发表于 2023-08-27 20:20:23">2023-08-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/" title="Attention机制_transformer"><img src="/images/Wallpaper/eatham.gif" onerror="this.onerror=null;this.src='/images/404.png'" alt="Attention机制_transformer"/></a><div class="content"><a class="title" href="/2022/11/21/Attention%E6%9C%BA%E5%88%B6-transformer/" title="Attention机制_transformer">Attention机制_transformer</a><time datetime="2022-11-21T11:55:15.000Z" title="发表于 2022-11-21 19:55:15">2022-11-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By LYC</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (true) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.isShuoshuo

  const loadLivere = (el, path) => {
    window.livereOptions = {
      refer: path || location.pathname
    }

    if (isShuoshuo) {
      window.shuoshuoComment.destroyLivere = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    if (typeof LivereTower === 'object') window.LivereTower.init()
    else {
      (function(d, s) {
          var j, e = d.getElementsByTagName(s)[0];
          if (typeof LivereTower === 'function') { return; }
          j = d.createElement(s);
          j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
          j.async = true;
          e.parentNode.insertBefore(j, e);
      })(document, 'script');
    }
  }

  if (isShuoshuo) {
    'Livere' === 'Livere'
      ? window.shuoshuoComment = { loadComment: loadLivere }
      : window.loadOtherComment = loadLivere
    return
  }

  if ('Livere' === 'Livere' || !false) {
    if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
    else loadLivere()
  } else {
    window.loadOtherComment = loadLivere
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>